## è¯´åœ¨å‰é¢
1. **k8s åªéœ€è¦å®‰è£…åœ¨ master èŠ‚ç‚¹ä¸Š**ï¼Œå…¶ä»–çš„èŠ‚ç‚¹éƒ½ä¸ç”¨  
2. kubeedge çš„è¿è¡Œå‰ææ˜¯ master ä¸Šå¿…é¡»æœ‰ k8s  
3. docker åªæ˜¯ç”¨æ¥å‘å¸ƒå®¹å™¨ pods çš„  
4. **calico åªéœ€è¦å®‰è£…åœ¨ master ä¸Š**ï¼Œå®ƒæ˜¯èŠ‚ç‚¹é€šä¿¡çš„æ’ä»¶ï¼Œå¦‚æœæ²¡æœ‰è¿™ä¸ªï¼Œmaster ä¸Šå®‰è£… kubeedge çš„ coredns ä¼šæŠ¥é”™ã€‚ä½†æ˜¯ï¼ŒèŠ‚ç‚¹ä¸Šåˆä¸éœ€è¦å®‰è£…è¿™ä¸ªï¼Œå› ä¸º kubeedge é’ˆå¯¹è¿™ä¸ªåšäº†è‡ªå·±çš„é€šä¿¡æœºåˆ¶  
5. ä¸€äº›æ’ä»¶æ¯”å¦‚ calicoã€edgemeshã€sednaã€metric-service è¿˜æœ‰ kuborad ç­‰ï¼Œéƒ½æ˜¯é€šè¿‡ yaml æ–‡ä»¶å¯åŠ¨çš„ï¼Œæ‰€ä»¥å®é™…è¦ä¸‹è½½çš„æ˜¯k8s çš„æ§åˆ¶å·¥å…· kubeadm å’Œ kubeedge çš„æ§åˆ¶å·¥å…· keadmã€‚ç„¶åæå‰å‡†å¤‡å¥½åˆšæ‰çš„ yaml æ–‡ä»¶ï¼Œå¯åŠ¨ k8s å’Œ kubeedge åï¼Œç›´æ¥æ ¹æ® yaml æ–‡ä»¶å®¹å™¨åˆ›å»º  
6. namespace å¯ä»¥çœ‹ä½œä¸åŒçš„è™šæ‹Ÿé¡¹ç›®ï¼Œservice æ˜¯æŒ‡å®šçš„ä»»åŠ¡ç›®æ ‡æ–‡ä»¶ï¼Œpods æ˜¯æ ¹æ® service æˆ–è€…å…¶ä»– yaml æ–‡ä»¶åˆ›å»ºçš„å…·ä½“å®¹å™¨  
7. ä¸€ä¸ªç‰©ç†èŠ‚ç‚¹ä¸Šå¯ä»¥æœ‰å¾ˆå¤šä¸ª podsï¼Œpods æ˜¯å¯æ“ä½œçš„æœ€å°å•ä½ï¼Œä¸€ä¸ª service å¯ä»¥è®¾ç½®å¾ˆå¤š podsï¼Œä¸€ä¸ª service å¯ä»¥åŒ…å«å¾ˆå¤šç‰©ç†èŠ‚ç‚¹  
8. ä¸€ä¸ª pods å¯ä»¥çœ‹ä½œä¸€ä¸ªæ ¹æ® docker é•œåƒåˆ›å»ºçš„å®ä¾‹  
9. **å¦‚æœæ˜¯ä¸»æœºå®¹å™¨åˆ›å»ºä»»åŠ¡ï¼Œè¦è®¾ç½® dnsPolicy**ï¼ˆå¾ˆé‡è¦ï¼‰  sedna-lc çš„å‘ç‚¹
10. æ‹‰å» docker é•œåƒçš„æ—¶å€™ï¼Œ**ä¸€å®šè¦å…ˆå»ç¡®è®¤æ¶æ„æ˜¯å¦æ”¯æŒ**
## å‰æœŸå‡†å¤‡
å‰æœŸå‡†å¤‡ä¸»è¦æ˜¯å‡†å¤‡ä¸€äº›å®‰è£…åŒ…ã€è®¾ç½®æºã€ä¸€äº› docker é…ç½®ç­‰ã€‚

### äº‘è¾¹å…±ç”¨

#### å…³é—­é˜²ç«å¢™
```
ufw disable	
```

#### å¼€å¯ ipv 4 è½¬å‘é…ç½® iptables å‚æ•°
å°†æ¡¥æ¥çš„ `IPv4/IPv6 ` æµé‡ä¼ é€’åˆ° iptables çš„é“¾
```
modprobe br_netfilter

cat >> /etc/sysctl.conf << EOF
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF
sysctl -p 
```

#### ç¦ç”¨ swap åˆ†åŒº
```bash
swapoff -a                                          #ä¸´æ—¶å…³é—­
sed -ri 's/.*swap.*/#&/' /etc/fstab                 #æ°¸ä¹…å…³é—­
```

#### è®¾ç½®ä¸»æœºå
```bash
hostnamectl set-hostname master   
bash

hostnamectl set-hostname edge1
bash

hostnamectl set-hostname edge2
bash
```

#### è®¾ç½® DNS
```bash
#ä¸åŒä¸»æœºä¸Šä½¿ç”¨ifconfigæŸ¥çœ‹ipåœ°å€
sudo vim /etc/hosts

#æ ¹æ®è‡ªå·±çš„ipæ·»åŠ 
#ip ä¸»æœºå
192.168.247.128 master
192.168.247.129 edge1
```

#### å®‰è£… docker å¹¶é…ç½®
```bash
sudo apt-get install docker.io -y

sudo systemctl start docker
sudo systemctl enable docker

docker --version

# å¦‚æœæ˜¯ ARM64 æ¶æ„
# å‚è€ƒ https://blog.csdn.net/qq_34253926/article/details/121629068
```

```bash
sudo vim /etc/docker/daemon.json

# 2024/6/28æ›´æ–°: dockerHubå›½å†…é•œåƒéƒ½å·²å¤±æ•ˆ,æ ¹æ® åˆ©ç”¨Cloudflareæ‹‰å– æ–‡æ¡£è¿›è¡Œé…ç½®æ‹‰å–ç›¸å…³çš„å†…å®¹
#masteræ·»åŠ ï¼š
{
  "registry-mirrors": ["https://b9pmyelo.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"]
}

#nodeæ·»åŠ ï¼š
{
  "registry-mirrors": ["https://b9pmyelo.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=cgroupfs"]
}

#éƒ½è¦æ‰§è¡Œ
sudo systemctl daemon-reload
sudo systemctl restart docker

#æŸ¥çœ‹ä¿®æ”¹åçš„docker Cgroupçš„å‚æ•°
docker info | grep Cgroup
```

### äº‘ç«¯
####  æ·»åŠ é˜¿é‡Œæº
```bash
curl -s https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | sudo apt-key add -
cat > /etc/apt/sources.list.d/kubernetes.list <<EOF
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt-get update
```

#### å®‰è£…ç›¸å…³ç»„ä»¶
éœ€è¦åœ¨æ¯å°æœºå™¨ä¸Šå®‰è£…ä»¥ä¸‹çš„è½¯ä»¶åŒ…ï¼š
	- `kubeadm`ï¼šç”¨æ¥åˆå§‹åŒ–é›†ç¾¤çš„æŒ‡ä»¤ã€‚  
	- `kubelet`ï¼šåœ¨é›†ç¾¤ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ä¸Šç”¨æ¥å¯åŠ¨ Pod å’Œå®¹å™¨ç­‰ã€‚  
	- `kubectl`ï¼šç”¨æ¥ä¸é›†ç¾¤é€šä¿¡çš„å‘½ä»¤è¡Œå·¥å…·ã€‚

```bash
#æŸ¥çœ‹è½¯ä»¶ç‰ˆæœ¬
apt-cache madison kubeadm

#å®‰è£…kubeadmã€kubeletã€kubectl
sudo apt-get update

# k8så’Œkubeedgeç‰ˆæœ¬å¯¹åº”è§ [kubeedge/kubeedge: Kubernetes Native Edge Computing Framework (project under CNCF) (github.com)](https://github.com/kubeedge/kubeedge)

sudo apt-get install -y kubelet=1.22.2-00 kubeadm=1.22.2-00 kubectl=1.22.2-00 
#é”å®šç‰ˆæœ¬
sudo apt-mark hold kubelet kubeadm kubectl
```

#### ä¸‹è½½é…ç½® calico
**å®˜æ–¹è§£é‡Š**ï¼šå› ä¸ºåœ¨æ•´ä¸ª kubernetes é›†ç¾¤é‡Œï¼Œpod éƒ½æ˜¯åˆ†å¸ƒåœ¨ä¸åŒçš„ä¸»æœºä¸Šçš„ï¼Œä¸ºäº†å®ç°è¿™äº› pod çš„è·¨ä¸»æœºé€šä¿¡æ‰€ä»¥æˆ‘ä»¬å¿…é¡»è¦å®‰è£… CNI ç½‘ç»œæ’ä»¶ï¼Œè¿™é‡Œé€‰æ‹© calico ç½‘ç»œã€‚
**å®è·µç»éªŒ**ï¼šåœ¨ k8s é›†ç¾¤åˆå§‹åŒ–åï¼Œå¦‚æœä¸é…ç½® calico ç­‰ç›¸å…³ç½‘ç»œæ’ä»¶ï¼Œä¼šå‘ç° coredns ä¸€ç›´å¤„åœ¨ pending æˆ– containerCreating çŠ¶æ€ï¼Œé€šè¿‡ describe ä¼šå‘ç°æç¤ºç¼ºå°‘ç½‘ç»œç»„ä»¶

**æ­¥éª¤ 1ï¼šåœ¨ master ä¸Šä¸‹è½½é…ç½® calico ç½‘ç»œçš„ yamlã€‚**
```bash
#æ³¨æ„å¯¹åº”ç‰ˆæœ¬ï¼Œv1.22å’Œv3.20
wget https://docs.projectcalico.org/v3.20/manifests/calico.yaml --no-check-certificate
```

**æ­¥éª¤ 2ï¼šä¿®æ”¹ calico.yaml é‡Œçš„ pod ç½‘æ®µã€‚**
```bash
#æŠŠcalico.yamlé‡Œpodæ‰€åœ¨ç½‘æ®µæ”¹æˆkubeadm initæ—¶é€‰é¡¹--pod-network-cidræ‰€æŒ‡å®šçš„ç½‘æ®µï¼Œ
#ç›´æ¥ç”¨vimç¼–è¾‘æ‰“å¼€æ­¤æ–‡ä»¶æŸ¥æ‰¾192ï¼ŒæŒ‰å¦‚ä¸‹æ ‡è®°è¿›è¡Œä¿®æ”¹ï¼š

# no effect. This should fall within `--cluster-cidr`.
# - name: CALICO_IPV4POOL_CIDR
#   value: "192.168.0.0/16"
# Disable file logging so `kubectl logs` works.
- name: CALICO_DISABLE_FILE_LOGGING
  value: "true"

#æŠŠä¸¤ä¸ª#åŠ#åé¢çš„ç©ºæ ¼å»æ‰ï¼Œå¹¶æŠŠ192.168.0.0/16æ”¹æˆ10.244.0.0/16

# no effect. This should fall within `--cluster-cidr`.
- name: CALICO_IPV4POOL_CIDR
  value: "10.244.0.0/16"
# Disable file logging so `kubectl logs` works.
- name: CALICO_DISABLE_FILE_LOGGING
  value: "true"
```

**æ­¥éª¤ 3ï¼šæå‰ä¸‹è½½æ‰€éœ€è¦çš„é•œåƒã€‚**
```bash
#æŸ¥çœ‹æ­¤æ–‡ä»¶ç”¨å“ªäº›é•œåƒï¼š
grep image calico.yaml

#image: calico/cni:v3.20.6
#image: calico/cni:v3.20.6
#image: calico/pod2daemon-flexvol:v3.20.6
#image: calico/node:v3.20.6
#image: calico/kube-controllers:v3.20.6
```

åœ¨ master èŠ‚ç‚¹ä¸­ä¸‹è½½é•œåƒ
```shell
#æ¢æˆè‡ªå·±çš„ç‰ˆæœ¬
for i in calico/cni:v3.20.6 calico/pod2daemon-flexvol:v3.20.6 calico/node:v3.20.6 calico/kube-controllers:v3.20.6 ; do docker pull $i ; done
```

==calico åªéœ€è¦åœ¨ master ä¸Šå­˜åœ¨==

 #### ä¸‹è½½metrics-server
 - ç”¨äºè¿½è¸ªè¾¹ç¼˜èŠ‚ç‚¹æ—¥å¿—
- å®˜ç½‘å®‰è£… (ä¼šå‡ºé”™, æ‹‰å–é•œåƒè¶…æ—¶é—®é¢˜)ï¼š
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```
![PixPin_2024-06-28_11-04-04](./img/PixPin_2024-06-28_11-04-04.png)

- æœ¬åœ°å®‰è£…

```bash
# å…ˆä¸‹è½½å®˜æ–¹æä¾›çš„yaml
wget https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

ä¿®æ”¹ yaml çš„å†…å®¹ï¼Œå…ˆçœ‹å®˜æ–¹çš„ç‰ˆæœ¬ï¼Œç„¶åå» docker hub æ‰¾å¯¹åº”çš„é•œåƒï¼Œ**å¹¶ä¸”æ·»åŠ â€œ- --kubelet-insecure-tlsâ€**

ä¿®æ”¹ `components.yaml` æ–‡ä»¶
```vim
    spec:
      containers:
      - args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        - --kubelet-insecure-tls
        image: mingyangtech/klogserver:v0.6.4

```

ç„¶åæ‰‹åŠ¨ pull é•œåƒï¼š`docker pull mingyangtech/klogserver:v0.6.4`

>æ›¿ä»£çš„é•œåƒæ˜¯å» dockerhub æœ metrics-server

![PixPin_2024-06-28_11-04-04](./img/PixPin_2024-06-28_11-05-36.png)

#### ä¸‹è½½ keadm

ç”¨äº kubeedge å®‰è£…å’Œä½¿ç”¨
è¿›å…¥å®˜ç½‘ç‰ˆæœ¬é“¾æ¥ï¼š[Release KubeEdge v1.9.2 release Â· kubeedge/kubeedge (github.com)](https://github.com/kubeedge/kubeedge/releases/tag/v1.9.2)
```bash
#æŸ¥çœ‹è‡ªå·±çš„æ¶æ„
uname -u

#ä¸‹è½½å¯¹åº”ç‰ˆæœ¬ä»¥åŠæ¶æ„
https://github.com/kubeedge/kubeedge/releases/download/v1.9.2/keadm-v1.9.2-linux-amd64.tar.gz
#è§£å‹
tar zxvf keadm-v1.9.2-linux-amd64.tar.gz
#æ·»åŠ æ‰§è¡Œæƒé™
chmod +x keadm-v1.9.2-linux-amd64/keadm/keadm 
#ç§»åŠ¨ç›®å½•
cp keadm-v1.9.2-linux-amd64/keadm/keadm /usr/local/bin/
```

#### ä¸‹è½½ Edgemesh
```bash
git clone https://github.com/kubeedge/edgemesh.git
```

#### ä¸‹è½½ Sedna
##### å®˜æ–¹ä¸‹è½½ï¼ˆä¼šè¶…æ—¶ï¼‰
```bash
curl https://raw.githubusercontent.com/kubeedge/sedna/main/scripts/installation/install.sh | SEDNA_ACTION=create bash -
```

![5](./img/5.png)

##### æœ¬åœ°ä¸‹è½½

- ä¸‹è½½ `install.sh` æ–‡ä»¶ï¼ˆä¸è¦ä¿å­˜åˆ° build ä¸­ï¼Œå¯ä»¥æ°¸ä¹…ä½¿ç”¨ï¼‰
```bash
wget https://raw.githubusercontent.com/kubeedge/sedna/main/scripts/installation/install.sh
```

- ç›¸å…³çš„ä¿®æ”¹ï¼ˆä¸è¦ä¿å­˜åˆ° build ä¸­ï¼Œå¯ä»¥æ°¸ä¹…ä½¿ç”¨ï¼‰
```bash
#æ”¹åå­—
mv install.sh offline-install.sh

#ä¿®æ”¹æ–‡ä»¶ä¸­çš„ä¸€ä¸‹å†…å®¹ï¼ˆshçš„è¯­æ³•ï¼‰
1.ç¬¬34è¡Œï¼šåˆ é™¤  trap "rm -rf '%TMP_DIR'" EXIT
2.ç¬¬415è¡Œï¼šåˆ é™¤  prepare
```


>[!warning]
>è®°å¾—çœ‹ `install.sh` ä¸­ sedna çš„ç‰ˆæœ¬ï¼Œå» docker hub ä¸­çœ‹é•œåƒæ˜¯å¦é€‚ç”¨äºä½ çš„æ¶æ„!! (ä¸»è¦é’ˆå¯¹ arm)


- ä¸‹è½½ sedna çš„å®˜ç½‘ githubï¼ˆä¸è¦ä¿å­˜åˆ° build ä¸­ï¼Œå¯ä»¥æ°¸ä¹…ä½¿ç”¨ï¼‰
å®˜æ–¹åœ°å€ï¼š[http://github.com/kubeedge/sed](https://link.zhihu.com/?target=http%3A//github.com/kubeedge/sedna/tree/main/build)
```bash
git clone https://github.com/kubeedge/sedna.git
```

**å› ä¸º sedna æˆ‘ä»¬åšè¿‡ä¸€å®šçš„ä¿®æ”¹ï¼Œæ‰€ä»¥ `git clone https://github.com/AdaYangOlzz/sedna-modified.git`ï¼Œå¹¶ä¸”è„šæœ¬çš„å†…å®¹åšäº†ä¸€å®šçš„ä¿®æ”¹ã€‚å¹¶ä¸”è®°å¾—æŠŠ lc çš„ä½¿ç”¨ä¸»æœºç½‘ç»œå»æ‰**

å®Œæ•´ç‰ˆ Sedna å®‰è£…è„šæœ¬ï¼š
```bash file:offline-install.sh
#!/bin/bash

# Copyright 2021 The KubeEdge Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Influential env vars:
#
# SEDNA_ACTION    | optional | 'create'/'clean', default is 'create'
# SEDNA_VERSION   | optional | The Sedna version to be installed.
#                              if not specified, it will get latest release version.
# SEDNA_ROOT      | optional | The Sedna offline directory

set -o errexit
set -o nounset
set -o pipefail

TMP_DIR='/opt/sedna'
SEDNA_ROOT='/home/hx/yby/software/kubeedge-1.9.2/sedna'

DEFAULT_SEDNA_VERSION=v0.3.1



get_latest_version() {
  # get Sedna latest release version
  local repo=kubeedge/sedna
  # output of this latest page:
  # ...
  # "tag_name": "v1.0.0",
  # ...
  {
    curl -s https://api.github.com/repos/$repo/releases/latest |
    awk '/"tag_name":/&&$0=$2' |
    sed 's/[",]//g'
  } || echo $DEFAULT_SEDNA_VERSION # fallback
}

: ${SEDNA_VERSION:=$(get_latest_version)}
SEDNA_VERSION=v${SEDNA_VERSION#v}

_download_yamls() {

  yaml_dir=$1
  mkdir -p ${SEDNA_ROOT}/$yaml_dir
  cd ${SEDNA_ROOT}/$yaml_dir
  for yaml in ${yaml_files[@]}; do
    # the yaml file already exists, no need to download
    [ -e "$yaml" ] && continue

    echo downloading $yaml into ${SEDNA_ROOT}/$yaml_dir
    local try_times=30 i=1 timeout=2
    while ! timeout ${timeout}s curl -sSO https://raw.githubusercontent.com/kubeedge/sedna/main/$yaml_dir/$yaml; do
      ((++i>try_times)) && {
        echo timeout to download $yaml
        exit 2
      }
      echo -en "retrying to download $yaml after $[i*timeout] seconds...\r"
    done
  done
}

download_yamls() {
  yaml_files=(
  sedna.io_datasets.yaml
  sedna.io_federatedlearningjobs.yaml
  sedna.io_incrementallearningjobs.yaml
  sedna.io_jointinferenceservices.yaml
  sedna.io_lifelonglearningjobs.yaml
  sedna.io_models.yaml
  )
  _download_yamls build/crds
  yaml_files=(
    gm.yaml
  )
  _download_yamls build/gm/rbac
}

prepare_install(){
  # need to create the namespace first
  kubectl create ns sedna
}

prepare() {
  mkdir -p ${SEDNA_ROOT}

  # we only need build directory
  # here don't use git clone because of large vendor directory
  download_yamls
}

cleanup(){
  kubectl delete ns sedna
}

create_crds() {
  cd ${SEDNA_ROOT}
  kubectl create -f build/crds
}

delete_crds() {
  cd ${SEDNA_ROOT}
  kubectl delete -f build/crds --timeout=90s
}

get_service_address() {
  local service=$1
  local port=$(kubectl -n sedna get svc $service -ojsonpath='{.spec.ports[0].port}')

  # <service-name>.<namespace>:<port>
  echo $service.sedna:$port
}

create_kb(){
  cd ${SEDNA_ROOT}

  kubectl $action -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: kb
  namespace: sedna
spec:
  selector:
    sedna: kb
  type: ClusterIP
  ports:
    - protocol: TCP
      port: 9020
      targetPort: 9020
      name: "tcp-0"  # required by edgemesh, to clean
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kb
  labels:
    sedna: kb
  namespace: sedna
spec:
  replicas: 1
  selector:
    matchLabels:
      sedna: kb
  template:
    metadata:
      labels:
        sedna: kb
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/edge
                operator: DoesNotExist
      serviceAccountName: sedna
      containers:
      - name: kb
        imagePullPolicy: IfNotPresent
        image: kubeedge/sedna-kb:$SEDNA_VERSION
        env:
          - name: KB_URL
            value: "sqlite:///db/kb.sqlite3"
        volumeMounts:
        - name: kb-url
          mountPath: /db
        resources:
          requests:
            memory: 256Mi
            cpu: 100m
          limits:
            memory: 512Mi
      volumes:
        - name: kb-url
          hostPath:
            path: /opt/kb-data
            type: DirectoryOrCreate
EOF
}

prepare_gm_config_map() {

  KB_ADDRESS=$(get_service_address kb)

  cm_name=${1:-gm-config}
  config_file=${TMP_DIR}/${2:-gm.yaml}

  if [ -n "${SEDNA_GM_CONFIG:-}" ] && [ -f "${SEDNA_GM_CONFIG}" ] ; then
    cp "$SEDNA_GM_CONFIG" $config_file
  else
    cat > $config_file << EOF
kubeConfig: ""
master: ""
namespace: ""
websocket:
  address: 0.0.0.0
  port: 9000
localController:
  server: http://localhost:${SEDNA_LC_BIND_PORT:-9100}
knowledgeBaseServer:
  server: http://$KB_ADDRESS
EOF
  fi

  kubectl $action -n sedna configmap $cm_name --from-file=$config_file
}

create_gm() {

  cd ${SEDNA_ROOT}

  kubectl create -f build/gm/rbac/

  cm_name=gm-config
  config_file_name=gm.yaml
  prepare_gm_config_map $cm_name $config_file_name


  kubectl $action -f - <<EOF
apiVersion: v1
kind: Service
metadata:
  name: gm
  namespace: sedna
spec:
  selector:
    sedna: gm
  type: ClusterIP
  ports:
    - protocol: TCP
      port: 9000
      targetPort: 9000
      name: "tcp-0"  # required by edgemesh, to clean
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gm
  labels:
    sedna: gm
  namespace: sedna
spec:
  replicas: 1
  selector:
    matchLabels:
      sedna: gm
  template:
    metadata:
      labels:
        sedna: gm
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/edge
                operator: DoesNotExist
      serviceAccountName: sedna
      containers:
      - name: gm
        image: dislabaiot.xyz/adayoung/sedna-gm:v0.3.11
        command: ["sedna-gm", "--config", "/config/$config_file_name", "-v2"]
        volumeMounts:
        - name: gm-config
          mountPath: /config
        resources:
          requests:
            memory: 32Mi
            cpu: 100m
          limits:
            memory: 256Mi
      volumes:
        - name: gm-config
          configMap:
            name: $cm_name
EOF
}

delete_gm() {
  cd ${SEDNA_ROOT}

  kubectl delete -f build/gm/rbac/

  # no need to clean gm deployment alone
}

create_lc() {

  GM_ADDRESS=$(get_service_address gm)

  kubectl $action -f- <<EOF
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    sedna: lc
  name: lc
  namespace: sedna
spec:
  selector:
    matchLabels:
      sedna: lc
  template:
    metadata:
      labels:
        sedna: lc
    spec:
      containers:
        - name: lc
          image: dislabaiot.xyz/adayoung/sedna-lc:v0.3.11
          env:
            - name: GM_ADDRESS
              value: $GM_ADDRESS
            - name: BIND_PORT
              value: "${SEDNA_LC_BIND_PORT:-9100}"
            - name: NODENAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: ROOTFS_MOUNT_DIR
              # the value of ROOTFS_MOUNT_DIR is same with the mount path of volume
              value: /rootfs
          resources:
            requests:
              memory: 32Mi
              cpu: 100m
            limits:
              memory: 128Mi
          volumeMounts:
            - name: localcontroller
              mountPath: /rootfs
      volumes:
        - name: localcontroller
          hostPath:
            path: /
      restartPolicy: Always
EOF
}

delete_lc() {
  # ns would be deleted in delete_gm
  # so no need to clean lc alone
  return
}

wait_ok() {
  echo "Waiting control components to be ready..."
  kubectl -n sedna wait --for=condition=available --timeout=600s deployment/gm
  kubectl -n sedna wait pod --for=condition=Ready --selector=sedna
  kubectl -n sedna get pod
}

delete_pods() {
  # in case some nodes are not ready, here delete with a 60s timeout, otherwise force delete these
  kubectl -n sedna delete pod --all --timeout=60s || kubectl -n sedna delete pod --all --force --grace-period=0
}

check_kubectl () {
  kubectl get pod >/dev/null
}

check_action() {
  action=${SEDNA_ACTION:-create}
  support_action_list="create delete"
  if ! echo "$support_action_list" | grep -w -q "$action"; then
    echo "\`$action\` not in support action list: create/delete!" >&2
    echo "You need to specify it by setting $(red_text SEDNA_ACTION) environment variable when running this script!" >&2
    exit 2
  fi

}

do_check() {
  check_kubectl
  check_action
}

show_debug_infos() {
  cat - <<EOF
Sedna is $(green_text running):
See GM status: kubectl -n sedna get deploy
See LC status: kubectl -n sedna get ds lc
See Pod status: kubectl -n sedna get pod
EOF
}

NO_COLOR='\033[0m'
RED='\033[0;31m'
GREEN='\033[0;32m'
green_text() {
  echo -ne "$GREEN$@$NO_COLOR"
}

red_text() {
  echo -ne "$RED$@$NO_COLOR"
}

do_check

case "$action" in
  create)
    echo "Installing Sedna $SEDNA_VERSION..."
    prepare_install
    create_crds
    create_kb
    create_gm
    create_lc
    wait_ok
    show_debug_infos
    ;;

  delete)
    # no errexit when fail to clean
    set +o errexit
    delete_pods
    delete_gm
    delete_lc
    delete_crds
    cleanup
    echo "$(green_text Sedna is uninstalled successfully)"
    ;;
esac

```

![4](./img/4.png)

![3](./img/3.png)

### è¾¹ç«¯

#### keadm
```bash
#æŸ¥çœ‹è‡ªå·±çš„æ¶æ„
uname -a

#ä¸‹è½½å¯¹åº”ç‰ˆæœ¬ä»¥åŠæ¶æ„
wget https://github.com/kubeedge/kubeedge/releases/download/v1.9.2/keadm-v1.9.2-linux-arm64.tar.gz
#è§£å‹
tar zxvf keadm-v1.9.2-linux-arm64.tar.gz
#æ·»åŠ æ‰§è¡Œæƒé™
chmod +x keadm-v1.9.2-linux-arm64/keadm/keadm 
#ç§»åŠ¨ç›®å½•
mv keadm-v1.9.2-linux-arm64/keadm/keadm /usr/local/bin/

```


## æ­£å¼å®‰è£…
### Kubernetes å¯åŠ¨(master)
è¯¥æ­¥å¯èƒ½ä¼šå‡ºç° [[#é—®é¢˜ä¸€ï¼škube-proxy æŠ¥ iptables é—®é¢˜]] ã€ [[#é—®é¢˜äºŒï¼šcalico å’Œ coredns ä¸€ç›´å¤„äºåˆå§‹åŒ–]]å’Œ[[#é—®é¢˜ä¸‰ï¼šmetrics-serverä¸€ç›´æ— æ³•æˆåŠŸ]]
##### åˆ é™¤ä»å‰çš„ä¿¡æ¯ï¼ˆå¦‚æœæ˜¯ç¬¬ä¸€æ¬¡å¯è·³è¿‡ï¼‰
```bash
swapoff -a
kubeadm reset
systemctl daemon-reload
systemctl restart docker kubelet
rm -rf $HOME/.kube/config

rm -f /etc/kubernetes/kubelet.conf    #åˆ é™¤k8sé…ç½®æ–‡ä»¶
rm -f /etc/kubernetes/pki/ca.crt    #åˆ é™¤K8Sè¯ä¹¦

rm -rf /etc/cni/net.d/  # åˆ é™¤ç½‘ç»œé…ç½®

iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
```

##### åˆå§‹åŒ– k8s é›†ç¾¤

```bash
kubeadm init --pod-network-cidr 10.244.0.0/16 \
--image-repository registry.cn-hangzhou.aliyuncs.com/google_containers

#ä¸Šä¸€æ­¥åˆ›å»ºæˆåŠŸåï¼Œä¼šæç¤ºï¼šä¸»èŠ‚ç‚¹æ‰§è¡Œæä¾›çš„ä»£ç 
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config

#å¦‚æœåªæœ‰ä¸€ä¸ªç‚¹ï¼Œéœ€è¦å®¹çº³æ±¡ç‚¹
kubectl taint nodes --all node-role.kubernetes.io/master node-role.kubernetes.io/master-

```

##### å¯åŠ¨ calico
```bash
ä¸€å®šè¦æ‰§è¡Œï¼Œä¸ç„¶corednsæ— æ³•åˆå§‹åŒ–
kubectl apply -f calico.yaml

#æ£€æŸ¥å¯åŠ¨çŠ¶æ€ï¼Œéœ€è¦éƒ½runningï¼Œé™¤äº†calicoå¯ä»¥ä¸ç”¨
kubectl get pods -A

```

##### å¯¹ calico å’Œ kube-proxy è¿›è¡Œé…ç½®
Edge èŠ‚ç‚¹åŠ å…¥æ—¶å¯èƒ½ä¼šè‡ªåŠ¨éƒ¨ç½² calico-node å’Œ kube-proxyï¼Œkube-proxy ä¼šéƒ¨ç½²æˆåŠŸï¼ˆä½†æ˜¯ edgecore çš„ log ä¼šæç¤ºä¸åº”è¯¥éƒ¨ç½² kube-proxyï¼‰ï¼Œcalico ä¼šåˆå§‹åŒ–å¤±è´¥ã€‚ä¸ºäº†é¿å…ä¸Šè¿°æƒ…å†µï¼Œåšå¦‚ä¸‹æ“ä½œï¼š

```bash
kubectl get daemonset -n kube-system | grep -v NAME | awk '{print $1}' |xargs -n 1 kubectl patch daemonset -n kube-system --type='json' -p='[{"op":"replace","path":"/spec/template/spec/affinity","value":{"nodeAffinity":{"requireDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchExpressions":[{"key":"node-role.kubernetes.io/edge","operator":"DoesNotExist"}]}]}}}}]'

$ kubectl edit daemonset -n kube-system kube-proxy

# æ·»åŠ ä»¥ä¸‹å†…å®¹
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/edge
                operator: DoesNotExist

# calico-nodeæ·»åŠ ç›¸åŒå†…å®¹
$ kubectl edit daemonset -n kube-system calico-node
```

æ­¤æ—¶è®°å¾— `kubectl get pods -A` çœ‹çœ‹æ‰€æœ‰çš„ pod æ˜¯å¦éƒ½åœ¨ runningã€‚æœ€åå¯ä»¥ logs ä¸€ä¸‹ï¼Œçœ‹çœ‹ç»„ä»¶å†…éƒ¨æ˜¯å¦æœ‰æŠ¥é”™ã€‚

<img src="./img/kube-proxy.png" alt="kube-proxy" style="zoom:60%;" />

##### å¯åŠ¨ metrics-server

```bash
# è¿›å…¥metrics-serverä¸‹è½½ç›®å½•
kubectl apply -f components.yaml

# æ£€éªŒæ˜¯å¦éƒ¨ç½²æˆåŠŸ
$ kubectl top nodes
```

### KubeEdge å¯åŠ¨

#### master
å¯èƒ½å‡ºç°[[#é—®é¢˜å››ï¼š10002 already in use]]
##### é‡å¯ï¼ˆå¦‚æœç¬¬ä¸€æ¬¡å¯ä»¥è·³è¿‡ï¼‰

```bash
keadm reset
```

##### å¯åŠ¨ cloudcore

é€šè¿‡ keadm å¯åŠ¨

```bash
#æ³¨æ„ä¿®æ”¹--advertise-addressçš„ipåœ°å€
keadm init --advertise-address=114.212.81.11 --kubeedge-version=1.9.2


#æ‰“å¼€è½¬å‘è·¯ç”±
kubectl get cm tunnelport -n kubeedge -o yaml
#æ‰¾åˆ°10350 æˆ–è€…10351
#set the rule of transï¼Œè®¾ç½®è‡ªå·±çš„ç«¯å£
export CLOUDCOREIPS=xxx.xxx.xxx.xxx

# dportçš„å†…å®¹å¯¹åº”tunnelport
iptables -t nat -A OUTPUT -p tcp --dport 10351 -j DNAT --to $CLOUDCOREIPS:10003

cd /etc/kubeedge/
cp $GOPATH/src/github.com/kubeedge/kubeedge/build/tools/certgen.sh /etc/kubeedge/

bash certgen.sh stream

```

##### cloudcore é…ç½®

```
$ vim /etc/kubeedge/config/cloudcore.yaml

modules:
  ..
  cloudStream:
    enable: true
    streamPort: 10003
  ..
  dynamicController:
    enable: true
```

é‡å¯ cloudcoreï¼š`sudo systemctl restart cloudcore.service`

è®°å¾— `journalclt -u cloudcore.service -xe` çœ‹çœ‹æ˜¯å¦æ­£å¸¸è¿è¡Œæ²¡æœ‰æŠ¥é”™
#### Node
å¯èƒ½å‡ºç°[[#é—®é¢˜äº”]]
##### resetï¼ˆå¦‚æœç¬¬ä¸€æ¬¡ï¼Œè¯·è·³è¿‡ï¼‰

```bash
#é‡å¯
keadm reset

#é‡æ–°åŠ å…¥æ—¶ï¼ŒæŠ¥é”™å­˜åœ¨æ–‡ä»¶å¤¹ï¼Œç›´æ¥åˆ é™¤
rm -rf /etc/kubeedge

#dockerå®¹å™¨å ç”¨(é€šå¸¸æ˜¯mqtt)
docker ps -a
docker stop mqtt
docker rm mqtt
```

##### å¯åŠ¨

```bash
#æ³¨æ„è¿™é‡Œæ˜¯è¦åŠ å…¥masterçš„IPåœ°å€ï¼Œtokenæ˜¯masterä¸Šè·å–çš„
keadm join --cloudcore-ipport=114.212.81.11:10000 --kubeedge-version=1.9.2 --token=9e1832528ae701aba2c4f7dfb49183ab2487e874c8090e68c19c95880cd93b50.eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJleHAiOjE3MTk1NjU4MzF9.1B4su4QwvQy_ZCPs-PIyDT9ixsDozfN1oG4vX59tKDs
```

```bash
#åŠ å…¥åï¼Œå…ˆæŸ¥çœ‹ä¿¡æ¯
journalctl -u edgecore.service -f
```

### EdgeMesh å¯åŠ¨
#### å‰æœŸå‡†å¤‡

å¯èƒ½å‡ºç°[[#é—®é¢˜å…­ï¼šç¼ºå°‘ TLSStreamCertFile]]

**æ­¥éª¤ 1**: å»é™¤ K8s master èŠ‚ç‚¹çš„æ±¡ç‚¹
```bash
kubectl taint nodes --all node-role.kubernetes.io/master-
```

**æ­¥éª¤ 2**: ç»™ Kubernetes API æœåŠ¡æ·»åŠ è¿‡æ»¤æ ‡ç­¾
```bash
kubectl label services kubernetes service.edgemesh.kubeedge.io/service-proxy-name=""
```

- **æ­¥éª¤ 3**: å¯ç”¨ KubeEdge çš„è¾¹ç¼˜ Kube-API ç«¯ç‚¹æœåŠ¡**important**  
  
- åœ¨äº‘ç«¯ï¼Œå¼€å¯ dynamicController æ¨¡å—ï¼Œé…ç½®å®Œæˆåï¼Œéœ€è¦é‡å¯ cloudcore `systemctl restart cloudcore`ï¼ˆè¿™ä¸€æ­¥åœ¨ cloudcore ä¸­é…ç½®è¿‡å¯è·³è¿‡ï¼‰
```bash
vim /etc/kubeedge/config/cloudcore.yaml
modules:
  ..
  cloudStream:
    enable: true
    streamPort: 10003
  ..
  dynamicController:
    enable: true
```

- åœ¨è¾¹ç«¯ï¼Œæ‰“å¼€ metaServer æ¨¡å—å’Œ edgeStream
```bash
$ vim /etc/kubeedge/config/edgecore.yaml
edgeStream:  
	enable: true  
	handshakeTimeout: 30  
	readDeadline: 15  
	server: 192.168.0.139:10004  
	tlsTunnelCAFile: /etc/kubeedge/ca/rootCA.crt  
	tlsTunnelCertFile: /etc/kubeedge/certs/server.crt  
	tlsTunnelPrivateKeyFile: /etc/kubeedge/certs/server.key  
	writeDeadline: 15
metaManager:
    metaServer:
      enable: true
...

#é‡å¯
systemctl restart edgecore.service
#æ£€æŸ¥çŠ¶å†µ
journalctl -u edgecore.service -f
#ç¡®å®šæ­£å¸¸è¿è¡Œ
```



**æ­¥éª¤ 4**: æœ€åï¼Œåœ¨è¾¹ç¼˜èŠ‚ç‚¹ï¼Œæµ‹è¯•è¾¹ç¼˜ Kube-API ç«¯ç‚¹åŠŸèƒ½æ˜¯å¦æ­£å¸¸

```bash
# è¾¹ç¼˜èŠ‚ç‚¹ä¸Š
curl 127.0.0.1:10550/api/v1/services
```

å¦‚æœè¿”å›å€¼æ˜¯ç©ºåˆ—è¡¨ï¼Œæˆ–è€…å“åº”æ—¶é•¿å¾ˆä¹…ï¼ˆæ¥è¿‘ 10 sï¼‰æ‰æ‹¿åˆ°è¿”å›å€¼ï¼Œè¯´æ˜ä½ çš„é…ç½®å¯èƒ½æœ‰è¯¯ï¼Œè¯·ä»”ç»†æ£€æŸ¥ã€‚

![api](./img/api.png)



#### å¯åŠ¨ EdgeMesh

æ­¤å¤„é—®é¢˜é‡ç¾åŒºã€‚~~è¯·ç†Ÿè¯»èƒŒè¯µ Q&A~~
>[!cite]
> [å¿«é€Ÿä¸Šæ‰‹ | EdgeMesh](https://edgemesh.netlify.app/zh/guide/#%E6%89%8B%E5%8A%A8%E5%AE%89%E8%A3%85)
> [å…¨ç½‘æœ€å…¨EdgeMesh Q&Aæ‰‹å†Œ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/585749690)


```
# ä¹‹å‰æœ‰ä¸‹å¥½çš„
cd edgemesh
```

- crd æ–‡ä»¶éƒ¨ç½²
```bash
kubectl apply -f build/crds/istio/
```

- éƒ¨ç½² edgemesh-agent
```bash
kubectl apply -f build/agent/resources/
```

- é…ç½® edge
```
vim /etc/kubeedge/config/edgecore.yaml

edged:
    clusterDNS: 169.254.96.16
    clusterDomain: cluster.local
```

![edgecore](./img/edgecore1.png)

![edgecore](./img/edgecore2.png)



#### æ­£å¸¸è¿è¡ŒçŠ¶æ€

iptablesï¼š

![iptables](./img/iptables.png)



äº‘ç«¯logsï¼š

![cloud-agent](./img/cloud-agent.png)



è¾¹ç«¯logsï¼š

![edge-agent.png](./img/edge-agent.png)



### å®‰è£… Sedna

#### å®‰è£…
```bash
#ç›´æ¥è¿›å…¥ä¹‹å‰å‡†å¤‡å¥½çš„æ–‡ä»¶ç›®å½•è¿è¡Œ
SEDNA_ACTION=create bash - offline-install.sh

# å¸è½½
SEDNA_ACTION=delete bash - offline-install.sh
```

å®‰è£…å®Œåéœ€è¦ç€é‡å»æŸ¥çœ‹ lc æ˜¯å¦è¿æ¥ gm æˆåŠŸã€‚é—®é¢˜é‡ç¾åŒº again

#### æ­£å¸¸è¿è¡Œæˆªå›¾

![lc](./img/lc.png)



![gm](./img/gm.png)

### GPU æ”¯æŒ

å…¶å®è¿™æ˜¯æ¢ç´¢å¾ˆè‰°éš¾çš„ä¸€ä¸ªè¿‡ç¨‹...å…ˆè®²æœ€ç»ˆç»“æœï¼š
#### å¼€å¯ GPU æ”¯æŒ
è¿è¡Œä¸‹é¢å‘½ä»¤å³å¯
```bash
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.13.0/nvidia-device-plugin.yml
```

>[!hint]
>åœ¨è¾¹ç«¯ `vim /etc/kubeedge/config/edgecore.yaml`
>å°†ä¿®æ”¹ä½¿å¾— `devicePluginEnabled: true`ã€‚æœ¬èº«è¿™ä¸ªæ’ä»¶æ˜¯æä¾›ç»™ `k8s` ä½¿ç”¨çš„ï¼Œæœ¬åœ°èŠ‚ç‚¹é‡‡ç”¨çš„ kubelet è¿›è¡Œç®¡ç†ï¼Œä½†æ˜¯ kubeedge é‡‡ç”¨ edgecoreï¼Œæ‰“å¼€ devicePluginEnabled æ‰å¯ä»¥



edgeçš„docker/daemon.json

```json
{
    "default-runtime": "nvidia",
    "runtimes": {
        "nvidia": {
            "path": "nvidia-container-runtime",
            "runtimeArgs": []
        }
    },
    "registry-mirrors": ["xxxxx"],
    "insecure-registries": ["xxxx"] // è®¾ç½®çš„å†…ç½‘ä»“åº“
}

```



cloudçš„docker/daemon.json

```json
{
    "exec-opts": [
        "native.cgroupdriver=systemd"
    ],
    "registry-mirrors": [
        "https://b9pmyelo.mirror.aliyuncs.com",
        "xxxx" // cloudflare æ‹‰å– é•œåƒ
    ],
    "default-runtime": "nvidia",
    "runtimes": {
        "nvidia": {
            "args": [],
            "path": "nvidia-container-runtime"
        }
    }
}

```





#### æ­£å¸¸è¿è¡Œæˆªå›¾

äº‘ç«¯ï¼š

<img src="./img/PixPin_2024-06-28_11-37-51.png" alt="PixPin_2024-06-28_11-37-51.png" style="zoom:67%;" />

![PixPin_2024-06-28_11-38-18.png](./img/PixPin_2024-06-28_11-38-18.png)

è¾¹ç«¯ï¼š

![PixPin_2024-06-28_11-39-07.png](./img/PixPin_2024-06-28_11-39-07.png)

![PixPin_2024-06-28_11-40-16.png](./img/PixPin_2024-06-28_11-40-16.png)



```bash
kubectl run -i -t nvidia --image=jitteam/devicequery
```

![PixPin_2024-06-28_11-40-44.png](./img/PixPin_2024-06-28_11-40-44.png)



#### å¯è·³è¿‡çš„æ¢ç´¢å†ç¨‹...

just è®°å½•ä¸€ä¸‹è‡ªå·±çš„è¾›è‹¦ğŸ’”
`k8s` ä½¿ç”¨ GPU éœ€è¦ nvidia-device-plugin æ’ä»¶æ”¯æŒ
```bash
kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.7.3/nvidia-device-plugin.yml
 
```

ä½†æ˜¯ä¼šå‘ç°é—®é¢˜ï¼š
![PixPin_2024-06-28_11-33-06.png](./img/PixPin_2024-06-28_11-33-06.png)
è¿™ä¸ªé—®é¢˜ä¹Ÿé‡è§äº†å¾ˆå¤šæ¬¡äº†ï¼Œå¤§æ¦‚å°±æ˜¯æ¶æ„é—®é¢˜å¯¼è‡´æ— æ³•è¿è¡Œï¼Œéœ€è¦é‡æ–°ç¼–è¯‘ dockerï¼Œäºæ˜¯å¼€å§‹å°è¯•

```bash
git clone -b 1.0.0-beta6 https://github.com/nvidia/k8s-device-plugin.git

cd k8s-device-plugin

wget https://labs.windriver.com/downloads/0001-arm64-add-support-for-arm64-architectures.patch

wget https://labs.windriver.com/downloads/0002-nvidia-Add-support-for-tegra-boards.patch

wget https://labs.windriver.com/downloads/0003-main-Add-support-for-tegra-boards.patch

git am 000*.patch

åº”ç”¨ï¼šarm64: add support for arm64 architectures
åº”ç”¨ï¼šnvidia: Add support for tegra boards
åº”ç”¨ï¼šmain: Add support for tegra boards

docker buildx  build --platform linux/arm64 -t adayoung/k8s-device-plugin:v0.1 -f docker/arm64/Dockerfile.ubuntu16.04 .
```

**å‡ºç°é—®é¢˜**ï¼š
![PixPin_2024-06-28_11-33-28.png](./img/PixPin_2024-06-28_11-33-28.png)

![PixPin_2024-06-28_11-33-48.png](./img/PixPin_2024-06-28_11-33-48.png)

![PixPin_2024-06-28_11-34-10.png](./img/PixPin_2024-06-28_11-34-10.png)

æ­¤é—®é¢˜çš„è§£å†³ï¼šåªéœ€è¦æ›´æ–°åŠ¨æ€åº“é…ç½®æ–‡ä»¶å’Œå°†å®ƒæŠ¥é”™çš„è·¯å¾„é“¾æ¥è¿‡å»

```bash
root@edge2:/software# ldconfig
root@edge2:/software# whereis /sbin/ldconfig
ldconfig: /sbin/ldconfig
root@edge2:/software# ln -s /sbin/ldconfig /sbin/ldconfig.real
root@edge2:/software# sudo systemctl daemon-reload
root@edge2:/software# sudo systemctl restart docker

```

ç„¶è€Œé—®é¢˜åªä¼šä¸€ä¸ªæ¥ä¸€ä¸ªçš„å‡ºç°ï¼š

![PixPin_2024-06-28_11-34-34.png](./img/PixPin_2024-06-28_11-34-34.png)

```bash
root@cloud:/home/guest/yby/kubeedge/multiedge/test-yaml/gpu# kubectl logs nvidia-device-plugin-daemonset-edge-65lbm -n kube-system
2024/01/04 06:08:15 Loading NVML
2024/01/04 06:08:15 Failed to initialize NVML: could not load NVML library.
2024/01/04 06:08:15 If this is a GPU node, did you set the docker default runtime to `nvidia`?
2024/01/04 06:08:15 You can check the prerequisites at: https://github.com/NVIDIA/k8s-device-plugin#prerequisites
2024/01/04 06:08:15 You can learn how to set the runtime at: https://github.com/NVIDIA/k8s-device-plugin#quick-start
2024/01/04 06:08:15 If this is not a GPU node, you should set up a toleration or nodeSelector to only deploy this plugin on GPU nodes
2024/01/04 06:08:15 Error: failed to initialize NVML: could not load NVML library

```

>[!quote]
> [nvidia-smi installation on Jetson TX2 - Jetson & Embedded Systems / Jetson TX2 - NVIDIA Developer Forums](https://forums.developer.nvidia.com/t/nvidia-smi-installation-on-jetson-tx2/111495)

![PixPin_2024-06-28_11-34-59.png](./img/PixPin_2024-06-28_11-34-59.png)

å®˜æ–¹å›å¤ jetson ç³»åˆ—ä¸èƒ½ç”¨ NVML...**ä½†æ˜¯æŸ³æš—èŠ±æ˜åˆä¸€æ‘!!è¿™ä¸ªæ’ä»¶å¼€å‘è€…å…¼å®¹äº† jetson æ¿å­ï¼**
>[!quote]
> [Add support for arm64 and Jetson boards. (!20) Â· åˆå¹¶è¯·æ±‚ Â· nvidia / kubernetes / device-plugin Â· GitLab](https://gitlab.com/nvidia/kubernetes/device-plugin/-/merge_requests/20)
> [NVIDIA/k8s-device-plugin at v0.13.0 (github.com)](https://github.com/NVIDIA/k8s-device-plugin/tree/v0.13.0)

ç„¶åå°±æ˜¯ [[#å¼€å¯ GPU æ”¯æŒ]]çš„éƒ¨åˆ†äº†


## å¯èƒ½å‡ºç°çš„é—®é¢˜
### é—®é¢˜ä¸€ï¼škube-proxy æŠ¥ iptables é—®é¢˜
```
E0627 09:28:54.054930 1 proxier.go:1598] Failed to execute iptables-restore: exit status 1 (iptables-restore: line 86 failed ) I0627 09:28:54.054962 1 proxier.go:879] Sync failed; retrying in 30s
```

è§£å†³ï¼šç›´æ¥æ¸…ç† iptables
```
iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X
```

### é—®é¢˜äºŒï¼šcalico å’Œ coredns ä¸€ç›´å¤„äºåˆå§‹åŒ–
ç”¨ `kubectl describe <podname>` åº”è¯¥ä¼šæœ‰ failed çš„æŠ¥é”™ï¼Œå¤§è‡´å†…å®¹æ˜¯è·Ÿ network å’Œ sandbox ç›¸å…³çš„ 
```
Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container "7f5b66ebecdfc2c206027a2afcb9d1a58ec5db1a6a10a91d4d60c0079236e401" network for pod "calico-kube-controllers-577f77cb5c-99t8z": networkPlugin cni failed to set up pod "calico-kube-controllers-577f77cb5c-99t8z_kube-system" network: error getting ClusterInformation: Get "https://[10.96.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default": dial tcp 10.96.0. 1:443: i/o timeout, failed to clean up sandbox container "7f5b66ebecdfc2c206027a2afcb9d1a58ec5db1a6a10a91d4d60c0079236e401" network for pod "calico-kube-controllers-577f77cb5c-99t8z": networkPlugin cni failed to teardown pod "calico-kube-controllers-577f77cb5c-99t8z_kube-system" network: error getting ClusterInformation: Get "https://[10.96.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default": dial tcp 10.96.0. 1:443: i/o timeout]
```

åŸå› ï¼šåº”è¯¥ä¸æ˜¯ç¬¬ä¸€æ¬¡åˆå§‹åŒ– k8s é›†ç¾¤ï¼Œæ‰€ä»¥ä¼šé‡åˆ°è¿™æ ·çš„é—®é¢˜ã€‚å‡ºç°çš„åŸå› æ˜¯ä¹‹å‰æ²¡æœ‰åˆ é™¤ k8s çš„ç½‘ç»œé…ç½®ï¼ˆæ–‡æ¡£é‡Œçš„æ¯ä¸€æ­¥éƒ½æœ‰å®ƒå­˜åœ¨çš„æ„ä¹‰...ï¼‰

è§£å†³ï¼š
```bash
rm -rf /etc/cni/net.d/  # åˆ é™¤ç½‘ç»œé…ç½®
# maybeéœ€è¦é‡æ–°initä¸€ä¸‹
```

### é—®é¢˜ä¸‰ï¼šmetrics-serverä¸€ç›´æ— æ³•æˆåŠŸ
åŸå› ï¼šmaster æ²¡æœ‰åŠ æ±¡ç‚¹
è§£å†³ï¼š`kubectl taint nodes --all node-role.kubernetes.io/master node-role.kubernetes.io/master-`

### é—®é¢˜å››ï¼š10002 already in use
`journalclt -u cloudcore.service -xe` æ—¶çœ‹åˆ° xxx already in use

åŸå› ï¼šåº”è¯¥æ˜¯ä¹‹å‰çš„è®°å½•æ²¡æœ‰æ¸…ç†å¹²å‡€
è§£å†³ï¼šæ‰¾åˆ°å ç”¨ç«¯å£çš„è¿›ç¨‹ï¼Œç›´æ¥ Kill å³å¯

```bash
lsof -i:xxxx
kill xxxxx
```

### é—®é¢˜äº”ï¼šedgecore ç¬¦å·é“¾æ¥å·²å­˜åœ¨
```
execute keadm command failed:  failed to exec 'bash -c sudo ln /etc/kubeedge/edgecore.service /etc/systemd/system/edgecore.service && sudo systemctl daemon-reload && sudo systemctl enable edgecore && sudo systemctl start edgecore', err: ln: failed to create hard link '/etc/systemd/system/edgecore.service': File exists
, err: exit status 1
```

åœ¨å°è¯•åˆ›å»ºç¬¦å·é“¾æ¥æ—¶ï¼Œç›®æ ‡è·¯å¾„å·²ç»å­˜åœ¨ï¼Œå› æ­¤æ— æ³•åˆ›å»ºã€‚è¿™é€šå¸¸æ˜¯å› ä¸º `edgecore.service` å·²ç»å­˜åœ¨äº `/etc/systemd/system/` ç›®å½•ä¸­
```bash
sudo rm /etc/systemd/system/edgecore.service
```

### é—®é¢˜å…­ï¼šç¼ºå°‘ TLSStreamCertFile
```bash
 TLSStreamPrivateKeyFile: Invalid value: "/etc/kubeedge/certs/stream.key": TLSStreamPrivateKeyFile not exist
12æœˆ 14 23:02:23 cloud.kubeedge cloudcore[196229]:   TLSStreamCertFile: Invalid value: "/etc/kubeedge/certs/stream.crt": TLSStreamCertFile not exist
12æœˆ 14 23:02:23 cloud.kubeedge cloudcore[196229]:   TLSStreamCAFile: Invalid value: "/etc/kubeedge/ca/streamCA.crt": TLSStreamCAFile not exist
12æœˆ 14 23:02:23 cloud.kubeedge cloudcore[196229]: ]

```

æŸ¥çœ‹ `/etc/kubeedge` ä¸‹æ˜¯å¦æœ‰ `certgen.sh` å¹¶ä¸” `bash certgen.sh stream`

### é—®é¢˜ä¸ƒï¼šedgemesh çš„ log è¾¹è¾¹äº’è”æˆåŠŸï¼Œäº‘è¾¹æ— æ³•è¿æ¥
#### æ’æŸ¥
å…ˆå¤ä¹ ä¸€ä¸‹**å®šä½æ¨¡å‹**ï¼Œç¡®å®š**è¢«è®¿é—®èŠ‚ç‚¹**ä¸Šçš„ edgemesh-agent(å³)å®¹å™¨æ˜¯å¦å­˜åœ¨ã€æ˜¯å¦å¤„äºæ­£å¸¸è¿è¡Œä¸­ã€‚

**è¿™ä¸ªæƒ…å†µæ˜¯éå¸¸ç»å¸¸å‡ºç°çš„**ï¼Œå› ä¸º master èŠ‚ç‚¹ä¸€èˆ¬éƒ½æœ‰æ±¡ç‚¹ï¼Œä¼šé©±é€å…¶ä»– podï¼Œè¿›è€Œå¯¼è‡´ edgemesh-agent éƒ¨ç½²ä¸ä¸Šå»ã€‚è¿™ç§æƒ…å†µå¯ä»¥é€šè¿‡å»é™¤èŠ‚ç‚¹æ±¡ç‚¹ï¼Œä½¿ edgemesh-agent éƒ¨ç½²ä¸Šå»è§£å†³ã€‚

å¦‚æœè®¿é—®èŠ‚ç‚¹å’Œè¢«è®¿é—®èŠ‚ç‚¹çš„ edgemesh-agent éƒ½æ­£å¸¸å¯åŠ¨äº†ï¼Œä½†æ˜¯è¿˜æŠ¥è¿™ä¸ªé”™è¯¯ï¼Œå¯èƒ½æ˜¯å› ä¸ºè®¿é—®èŠ‚ç‚¹å’Œè¢«è®¿é—®èŠ‚ç‚¹æ²¡æœ‰äº’ç›¸å‘ç°å¯¼è‡´ï¼Œè¯·è¿™æ ·æ’æŸ¥ï¼š

1. é¦–å…ˆæ¯ä¸ªèŠ‚ç‚¹ä¸Šçš„ edgemesh-agent éƒ½å…·æœ‰ peer IDï¼Œæ¯”å¦‚

```bash

edge2: 
I'm {12D3KooWPpY4GqqNF3sLC397fMz5ZZfxmtMTNa1gLYFopWbHxZDt: [/ip4/127.0.0.1/tcp/20006 /ip4/192.168.1.4/tcp/20006]}

edge1.kubeedge:
I'm {12D3KooWFz1dKY8L3JC8wAY6sJ5MswvPEGKysPCfcaGxFmeH7wkz: [/ip4/127.0.0.1/tcp/20006 /ip4/192.168.1.2/tcp/20006]}

æ³¨æ„ï¼š
a. peer IDæ˜¯æ ¹æ®èŠ‚ç‚¹åç§°å“ˆå¸Œå‡ºæ¥çš„ï¼Œç›¸åŒçš„èŠ‚ç‚¹åç§°ä¼šå“ˆå¸Œå‡ºç›¸åŒçš„peer ID
b. å¦å¤–ï¼ŒèŠ‚ç‚¹åç§°ä¸æ˜¯æœåŠ¡å™¨åç§°ï¼Œæ˜¯k8s node nameï¼Œè¯·ç”¨kubectl get nodesæŸ¥çœ‹
```

2.å¦‚æœè®¿é—®èŠ‚ç‚¹å’Œè¢«è®¿é—®èŠ‚ç‚¹å¤„äºåŒä¸€ä¸ªå±€åŸŸç½‘å†…ï¼ˆ**æ‰€æœ‰èŠ‚ç‚¹åº”è¯¥å…·å¤‡å†…ç½‘ IPï¼ˆ10.0.0.0/8ã€172.16.0.0/12ã€192.168.0.0/16**ï¼‰ï¼Œè¯·çœ‹[å…¨ç½‘æœ€å…¨EdgeMesh Q&Aæ‰‹å†Œ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/585749690)**é—®é¢˜åäºŒ**åŒä¸€ä¸ªå±€åŸŸç½‘å†… edgemesh-agent äº’ç›¸å‘ç°å¯¹æ–¹æ—¶çš„æ—¥å¿—æ˜¯Â `[MDNS] Discovery found peer: <è¢«è®¿é—®ç«¯peer ID: [è¢«è®¿é—®ç«¯IPåˆ—è¡¨(å¯èƒ½ä¼šåŒ…å«ä¸­ç»§èŠ‚ç‚¹IP)]>` ^d3939d

3.å¦‚æœè®¿é—®èŠ‚ç‚¹å’Œè¢«è®¿é—®èŠ‚ç‚¹è·¨å­ç½‘ï¼Œè¿™æ—¶å€™åº”è¯¥çœ‹çœ‹ relayNodes è®¾ç½®çš„æ­£ä¸æ­£ç¡®ï¼Œä¸ºä»€ä¹ˆä¸­ç»§èŠ‚ç‚¹æ²¡åŠæ³•ååŠ©ä¸¤ä¸ªèŠ‚ç‚¹äº¤æ¢ peer ä¿¡æ¯ã€‚è¯¦ç»†ææ–™è¯·é˜…è¯»ï¼š[KubeEdge EdgeMesh é«˜å¯ç”¨æ¶æ„è¯¦è§£](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/4whnkMM9oOaWRsI1ICsvSA)ã€‚è·¨å­ç½‘çš„ edgemesh-agent äº’ç›¸å‘ç°å¯¹æ–¹æ—¶çš„æ—¥å¿—æ˜¯Â `[DHT] Discovery found peer: <è¢«è®¿é—®ç«¯peer ID: [è¢«è®¿é—®ç«¯IPåˆ—è¡¨(å¯èƒ½ä¼šåŒ…å«ä¸­ç»§èŠ‚ç‚¹IP)]>`ï¼ˆé€‚ç”¨äºæˆ‘çš„æƒ…å†µï¼‰

#### è§£å†³
åœ¨éƒ¨ç½² edgemesh è¿›è¡Œ `kubectl apply -f build/agent/resources/` æ“ä½œæ—¶ï¼Œä¿®æ”¹ 04-configmapï¼Œæ·»åŠ  relayNodeï¼ˆæ ¹æœ¬åŸå› åœ¨äºï¼Œä¸ç¬¦åˆâ€œè®¿é—®èŠ‚ç‚¹å’Œè¢«è®¿é—®èŠ‚ç‚¹å¤„äºåŒä¸€ä¸ªå±€åŸŸç½‘å†…â€ï¼Œæ‰€ä»¥éœ€è¦æ·»åŠ  relayNodeï¼‰

<img src="./img/relayNode.png" alt="relayNode.png" style="zoom:67%;" />

### é—®é¢˜å…«ï¼šmaster çš„gpu å­˜åœ¨ä½†æ˜¯æ‰¾ä¸åˆ° gpu èµ„æº
ä¸»è¦é’ˆå¯¹çš„æ˜¯æœåŠ¡å™¨çš„æƒ…å†µï¼Œå¯ä»¥ä½¿ç”¨ `nvidia-smi` æŸ¥çœ‹æ˜¾å¡æƒ…å†µã€‚

>[!quote]
> [Installing the NVIDIA Container Toolkit â€” NVIDIA Container Toolkit 1.14.3 documentation](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuration)

æŒ‰ä¸Šè¿°å†…å®¹è¿›è¡Œé…ç½®ï¼Œç„¶åè¿˜éœ€è¦ `vim /etc/docker/daemon.json`ï¼Œæ·»åŠ  default-runtimeã€‚æŒ‰ç…§ quote çš„å†…å®¹è®¾ç½®åä¼šæœ‰â€œruntimesâ€ä½†æ˜¯ default-runtime ä¸ä¼šè®¾ç½®ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ‰¾ä¸åˆ° GPU èµ„æº
```
{
    "exec-opts": [
        "native.cgroupdriver=systemd"
    ],
    "registry-mirrors": [
        "https://b9pmyelo.mirror.aliyuncs.com"
    ],
    "default-runtime": "nvidia",
    "runtimes": {
        "nvidia": {
            "args": [],
            "path": "nvidia-container-runtime"
        }
    }
}

```

### é—®é¢˜ä¹ï¼šjeston çš„ gpu å­˜åœ¨ä½†æ˜¯æ‰¾ä¸åˆ° gpu èµ„æº
ç†è®ºä¸Š `k8s-device-plugin` å·²ç»æ”¯æŒäº† tegra å³ jetson ç³»åˆ—æ¿å­ï¼Œä¼šåœ¨æŸ¥çœ‹ GPU ä¹‹å‰åˆ¤æ–­æ˜¯å¦æ˜¯ tegra æ¶æ„ï¼Œå¦‚æœæ˜¯åˆ™é‡‡ç”¨ tegra ä¸‹æŸ¥çœ‹ GPU çš„æ–¹å¼ï¼ˆåŸå› åœ¨ [[#GPU æ”¯æŒ]]é‡Œ quote è¿‡äº† ï¼‰ï¼Œä½†æ˜¯å¾ˆå¥‡æ€ªçš„æ˜¯æ˜æ˜æ˜¯ tegra çš„æ¶æ„å´æ²¡æœ‰æ£€æµ‹åˆ°ï¼š
```bash
2024/01/04 07:43:58 Retreiving plugins.
2024/01/04 07:43:58 Detected non-NVML platform: could not load NVML: libnvidia-ml.so.1: cannot open shared object file: No such file or directory
2024/01/04 07:43:58 Detected non-Tegra platform: /sys/devices/soc0/family file not found
2024/01/04 07:43:58 Incompatible platform detected
2024/01/04 07:43:58 If this is a GPU node, did you configure the NVIDIA Container Toolkit?
2024/01/04 07:43:58 You can check the prerequisites at: https://github.com/NVIDIA/k8s-device-plugin#prerequisites
2024/01/04 07:43:58 You can learn how to set the runtime at: https://github.com/NVIDIA/k8s-device-plugin#quick-start
2024/01/04 07:43:58 If this is not a GPU node, you should set up a toleration or nodeSelector to only deploy this plugin on GPU nodes
2024/01/04 07:43:58 No devices found. Waiting indefinitely.

```

![PixPin_2024-06-28_11-28-29.png](./img/PixPin_2024-06-28_11-28-29.png)

```bash
$ dpkg -l '*nvidia*'
```

![PixPin_2024-06-28_11-29-20.png](./img/PixPin_2024-06-28_11-29-20.png)

![PixPin_2024-06-28_11-29-40.png](./img/PixPin_2024-06-28_11-29-40.png)

>[!quote]
> [Plug in does not detect Tegra device Jetson Nano Â· Issue #377 Â· NVIDIA/k8s-device-plugin (github.com)](https://github.com/NVIDIA/k8s-device-plugin/issues/377)
>
>Note that looking at the initial logs that you provided you may have been usingÂ `v1.7.0`Â of the NVIDIA Container Toolkit. This is quite an old version and we greatly improved our support for Tegra-based systems with theÂ `v1.10.0`Â release. It should also be noted that in order to use the GPU Device Plugin on Tegra-based systems (specifically targetting the integrated GPUs) at leastÂ `v1.11.0`Â of the NVIDIA Container Toolkit is required.
>
>There are no Tegra-specific changes in theÂ `v1.12.0`Â release, so using theÂ `v1.11.0`Â release should be sufficient in this case.

é‚£ä¹ˆåº”è¯¥éœ€è¦å‡çº§**NVIDIA Container Toolkit**

```bash
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update

sudo apt-get install -y nvidia-container-toolkit
```

### é—®é¢˜åï¼šlc127.0.0. 53:53 no such host/connection refused
åŸå› ï¼šè§£æ gm.sedna è¿™ä¸ªåŸŸåå¤±è´¥
è§£å†³ï¼š
1. å¦‚æœå®‰è£… sedna è„šæœ¬æœ‰å°† hostNetwork å»æ‰ï¼Œåˆ™æ£€æŸ¥ edgecore.yaml çš„ clusterDNS éƒ¨åˆ†ï¼Œ**ç€é‡æ³¨æ„æ˜¯å¦æ²¡æœ‰äºŒæ¬¡è®¾ç½®ç„¶åè¢«åè®¾ç½®çš„è¦†ç›–æ‰**
2. å¦‚æœæ²¡æœ‰å°† hostNetwork å»æ‰ï¼Œåˆ™å°†å®¿ä¸»æœºçš„ `/etc/resolv.conf` æ·»åŠ  `nameserver 169.254.96.16`

### é—®é¢˜åä¸€ï¼š169.254.96.16:no such host
æ£€æŸ¥ edgemesh çš„é…ç½®æ˜¯å¦æ­£ç¡®ï¼š
1. æ£€æŸ¥ iptables çš„é“¾çš„é¡ºåºå¦‚[æ··åˆä»£ç† | EdgeMesh](https://edgemesh.netlify.app/zh/advanced/hybird-proxy.html) æ‰€ç¤º
2. ç€é‡æ£€æŸ¥ clusterDNS



### é—®é¢˜åäºŒï¼š `kubectl logs <pod-name>` è¶…æ—¶

![timeout](./img/timeout.png)

åŸå› ï¼šå€Ÿé‰´ [Kubernetes è¾¹ç¼˜èŠ‚ç‚¹æŠ“ä¸åˆ°ç›‘æ§æŒ‡æ ‡ï¼Ÿè¯•è¯•è¿™ä¸ªæ–¹æ³•ï¼ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/379962934)

![PixPin_2024-06-28_11-24-28.png](./img/PixPin_2024-06-28_11-24-28.png)

å¯ä»¥å‘ç°æŠ¥é”™è®¿é—®çš„ç«¯å£å°±æ˜¯ 10350ï¼Œè€Œåœ¨ kubeedge ä¸­ 10350 åº”è¯¥ä¼šè¿›è¡Œè½¬å‘ï¼Œæ‰€ä»¥åº”è¯¥æ˜¯ cloudcore çš„è®¾ç½®é—®é¢˜ã€‚

è§£å†³ï¼š[Enable Kubectl logs/exec to debug pods on the edge | KubeEdge](https://kubeedge.io/docs/advanced/debug/) æ ¹æ®è¿™ä¸ªé“¾æ¥è®¾ç½®å³å¯ã€‚



### é—®é¢˜åä¸‰ï¼š `kubectl logs <pod-name>` å¡ä½ 

å¯èƒ½çš„åŸå› ï¼šä¹‹å‰ `kubectl logs` æ—¶æœªç»“æŸå°± ctrl+c ç»“æŸäº†å¯¼è‡´åç»­å¡ä½
è§£å†³ï¼šé‡å¯ edgecore/cloudcore `systemctl restart edgecore.service`



### é—®é¢˜åå››ï¼šCloudCoreæŠ¥certficateé”™è¯¯

![PixPin_2024-06-28_11-26-15.png](./img/PixPin_2024-06-28_11-26-15.png)

åŸå› ï¼šå› ä¸ºæ˜¯é‡è£…ï¼Œä¸»èŠ‚ç‚¹ token å˜äº†ï¼Œä½†æ˜¯è¾¹ç¼˜èŠ‚ç‚¹ä¸€ç›´ä»¥è¿‡å»çš„ token å°è¯•è¿›è¡Œè¿æ¥

è§£å†³ï¼šè¾¹ç«¯ç”¨æ–°çš„ token è¿æ¥å°±å¥½



###  é—®é¢˜åäº”ï¼šåˆ é™¤å‘½åç©ºé—´å¡åœ¨ terminating

ç†è®ºä¸Šä¸€ç›´ç­‰å¾…åº”è¯¥æ˜¯å¯ä»¥çš„(ä½†æ˜¯æˆ‘ç­‰äº†åŠä¸ªé’Ÿä¹Ÿæ²¡æˆåŠŸå•Š!!)
**æ–¹æ³•ä¸€**ï¼Œä½†æ˜¯æ²¡å•¥ç”¨ï¼Œä¾æ—§å¡ä½

```bash
kubectl delete ns sedna --force --grace-period=0
```

**æ–¹æ³•äºŒ**ï¼š
```bash
å¼€å¯ä¸€ä¸ªä»£ç†ç»ˆç«¯
$ kubectl proxy
Starting to serve on 127.0.0.1:8001

å†å¼€å¯ä¸€ä¸ªæ“ä½œç»ˆç«¯
å°†test namespaceçš„é…ç½®æ–‡ä»¶è¾“å‡ºä¿å­˜
$ kubectl get ns sedna -o json > sedna.json
åˆ é™¤specåŠstatuséƒ¨åˆ†çš„å†…å®¹è¿˜æœ‰metadataå­—æ®µåçš„","å·ï¼Œåˆ‡è®°ï¼
å‰©ä¸‹å†…å®¹å¤§è‡´å¦‚ä¸‹
guest@cloud:~/yby$ cat sedna.json
{
    "apiVersion": "v1",
    "kind": "Namespace",
    "metadata": {
        "creationTimestamp": "2023-12-14T09:12:13Z",
        "deletionTimestamp": "2023-12-14T09:15:25Z",
        "managedFields": [
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:phase": {}
                    }
                },
                "manager": "kubectl-create",
                "operation": "Update",
                "time": "2023-12-14T09:12:13Z"
            },
            {
                "apiVersion": "v1",
                "fieldsType": "FieldsV1",
                "fieldsV1": {
                    "f:status": {
                        "f:conditions": {
                            ".": {},
                            "k:{\"type\":\"NamespaceContentRemaining\"}": {
                                ".": {},
                                "f:lastTransitionTime": {},
                                "f:message": {},
                                "f:reason": {},
                                "f:status": {},
                                "f:type": {}
                            },
                            "k:{\"type\":\"NamespaceDeletionContentFailure\"}": {
                                ".": {},
                                "f:lastTransitionTime": {},
                                "f:message": {},
                                "f:reason": {},
                                "f:status": {},
                                "f:type": {}
                            },
                            "k:{\"type\":\"NamespaceDeletionDiscoveryFailure\"}": {
                                ".": {},
                                "f:lastTransitionTime": {},
                                "f:message": {},
                                "f:reason": {},
                                "f:status": {},
                                "f:type": {}
                            },
                            "k:{\"type\":\"NamespaceDeletionGroupVersionParsingFailure\"}": {
                                ".": {},
                                "f:lastTransitionTime": {},
                                "f:message": {},
                                "f:reason": {},
                                "f:status": {},
                                "f:type": {}
                            },
                            "k:{\"type\":\"NamespaceFinalizersRemaining\"}": {
                                ".": {},
                                "f:lastTransitionTime": {},
                                "f:message": {},
                                "f:reason": {},
                                "f:status": {},
                                "f:type": {}
                            }
                        }
                    }
                },
                "manager": "kube-controller-manager",
                "operation": "Update",
                "time": "2023-12-14T09:15:30Z"
            }
        ],
        "name": "sedna",
        "resourceVersion": "3351515",
        "uid": "99cb8afb-a4c1-45e6-960d-ff1b4894773d"
    }
}


è°ƒæ¥å£åˆ é™¤
# curl -k -H "Content-Type: application/json" -X PUT --data-binary @sedna.json http://127.0.0.1:8001/api/v1/namespaces/sedna/finalize
{
  "kind": "Namespace",
  "apiVersion": "v1",
  "metadata": {
    "name": "sedna",
    "uid": "99cb8afb-a4c1-45e6-960d-ff1b4894773d",
    "resourceVersion": "3351515",
    "creationTimestamp": "2023-12-14T09:12:13Z",
    "deletionTimestamp": "2023-12-14T09:15:25Z",
    "managedFields": [
      {
        "manager": "curl",
        "operation": "Update",
        "apiVersion": "v1",
        "time": "2023-12-14T09:42:38Z",
        "fieldsType": "FieldsV1",
        "fieldsV1": {"f:status":{"f:phase":{}}}
      }
    ]
  },
  "spec": {

  },
  "status": {
    "phase": "Terminating",
    "conditions": [
      {
        "type": "NamespaceDeletionDiscoveryFailure",
        "status": "True",
        "lastTransitionTime": "2023-12-14T09:15:30Z",
        "reason": "DiscoveryFailed",
        "message": "Discovery failed for some groups, 1 failing: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: the server is currently unable to handle the request"
      },
      {
        "type": "NamespaceDeletionGroupVersionParsingFailure",
        "status": "False",
        "lastTransitionTime": "2023-12-14T09:15:30Z",
        "reason": "ParsedGroupVersions",
        "message": "All legacy kube types successfully parsed"
      },
      {
        "type": "NamespaceDeletionContentFailure",
        "status": "False",
        "lastTransitionTime": "2023-12-14T09:15:30Z",
        "reason": "ContentDeleted",
        "message": "All content successfully deleted, may be waiting on finalization"
      },
      {
        "type": "NamespaceContentRemaining",
        "status": "False",
        "lastTransitionTime": "2023-12-14T09:15:30Z",
        "reason": "ContentRemoved",
        "message": "All content successfully removed"
      },
      {
        "type": "NamespaceFinalizersRemaining",
        "status": "False",
        "lastTransitionTime": "2023-12-14T09:15:30Z",
        "reason": "ContentHasNoFinalizers",
        "message": "All content-preserving finalizers finished"
      }
    ]
  }
}

```

### é—®é¢˜åå…­ï¼šå¼ºåˆ¶åˆ é™¤ pod ä¹‹åéƒ¨ç½²ä¸æˆåŠŸ

#### é—®é¢˜æè¿°
å› ä¸ºç°åœ¨ edge çš„ pod æ˜¯é€šè¿‡åˆ›å»º deployment ç”± deployment è¿›è¡Œåˆ›å»ºï¼Œä½†æ˜¯é€šè¿‡ `kubectl delete deploy <deploy-name>` åˆ é™¤ deployment åï¼Œpod ä¸€ç›´å¡åœ¨äº† terminating çŠ¶æ€ï¼Œäºæ˜¯é‡‡ç”¨äº† `kubectl delete pod edgeworker-deployment-7g5hs-58dffc5cd7-b77wz --force --grace-period=0` å‘½ä»¤è¿›è¡Œäº†åˆ é™¤ã€‚
ç„¶åå‘ç°é‡æ–°éƒ¨ç½²æ—¶å€™å‘ç° assigned to edge çš„ pod éƒ½å¡åœ¨ pending çŠ¶æ€ã€‚

#### è§£å†³
å› ä¸º--force æ˜¯ä¸ä¼šå®é™…ç»ˆæ­¢è¿è¡Œçš„ï¼Œæ‰€ä»¥æœ¬èº«åŸæ¥çš„ docker å¯èƒ½è¿˜åœ¨è¿è¡Œï¼Œç°åœ¨çš„åšæ³•æ˜¯æ‰‹åŠ¨å»å¯¹åº”çš„è¾¹ç¼˜èŠ‚ç‚¹ä¸Šåˆ é™¤å¯¹åº”çš„å®¹å™¨ï¼ˆåŒ…æ‹¬ pauseï¼Œå…³äº pause å¯ä»¥çœ‹è¿™ç¯‡æ–‡ç« [å¤§ç™½è¯ K8Sï¼ˆ03ï¼‰ï¼šä» Pause å®¹å™¨ç†è§£ Pod çš„æœ¬è´¨ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/464712164)ï¼‰ï¼Œç„¶åé‡å¯ edgecore: `systemctl restart edgecore.service`

### é—®é¢˜åä¸ƒï¼šåˆ é™¤ deploymentã€pod ç­‰ï¼Œå®¹å™¨ä¾æ—§è‡ªåŠ¨é‡å¯
```
 journalctl -u edgecore.service  -f
```

![[Pasted image 20240313150604.png]]

è§£å†³ï¼šé‡å¯ edgecore
```
systemctl restart edgecore.service
```

### é—®é¢˜åå…«ï¼šå¤§é¢ç§¯ Evictedï¼ˆdisk pressureï¼‰
#### åŸå› 
- node ä¸Šçš„ kubelet è´Ÿè´£é‡‡é›†èµ„æºå ç”¨æ•°æ®ï¼Œå¹¶å’Œé¢„å…ˆè®¾ç½®çš„ threshold å€¼è¿›è¡Œæ¯”è¾ƒï¼Œå¦‚æœè¶…è¿‡ threshold å€¼ï¼Œkubelet ä¼šæ€æ‰ä¸€äº› Pod æ¥å›æ”¶ç›¸å…³èµ„æºï¼Œ[K8sgå®˜ç½‘è§£è¯»kubernetesé…ç½®èµ„æºä¸è¶³å¤„ç†](https://links.jianshu.com/go?to=https%3A%2F%2Fkubernetes.io%2Fdocs%2Ftasks%2Fadminister-cluster%2Fout-of-resource%2F)
  
- é»˜è®¤å¯åŠ¨æ—¶ï¼Œnode çš„å¯ç”¨ç©ºé—´ä½äº15%çš„æ—¶å€™ï¼Œè¯¥èŠ‚ç‚¹ä¸Šè®²ä¼šæ‰§è¡Œ eviction æ“ä½œï¼Œç”±äºç£ç›˜å·²ç»è¾¾åˆ°äº†85%,åœ¨æ€ä¹ˆé©±é€ä¹Ÿæ— æ³•æ­£å¸¸å¯åŠ¨å°±ä¼šä¸€ç›´é‡å¯ï¼ŒPod çŠ¶æ€ä¹Ÿæ˜¯ pending ä¸­

 #### ä¸´æ—¶è§£å†³æ–¹æ³•
- ä¿®æ”¹é…ç½®æ–‡ä»¶å¢åŠ ä¼ å‚æ•°,æ·»åŠ æ­¤é…ç½®é¡¹`--eviction-hard=nodefs.available<5%`
```bash
root@cloud:/usr/lib/systemd/system# systemctl status kubelet
â— kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             â””â”€10-kubeadm.conf
     Active: active (running) since Fri 2023-12-15 09:17:50 CST; 5min ago
       Docs: https://kubernetes.io/docs/home/
   Main PID: 1070209 (kubelet)
      Tasks: 59 (limit: 309024)
     Memory: 67.2M
     CGroup: /system.slice/kubelet.service

```

å¯ä»¥çœ‹åˆ°é…ç½®æ–‡ä»¶ç›®å½•æ‰€åœ¨ä½ç½®æ˜¯ `/etc/systemd/system/kubelet.service.d`ï¼Œé…ç½®æ–‡ä»¶æ˜¯ `10-kubeadm.conf`
```bash
vim /etc/systemd/system/kubelet.service.d/10-kubeadm.conf

[Service]
Environment="KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf"
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml --eviction-hard=nodefs.available<5%"

æœ€åæ·»åŠ ä¸Š--eviction-hard=nodefs.available<5%
```

ç„¶åé‡å¯ kubelet
```bash
systemctl daemon-reload
systemctl  restart kubelet
```

ä¼šå‘ç°å¯ä»¥æ­£å¸¸éƒ¨ç½²äº†(ä½†æ˜¯åªæ˜¯æ€¥æ•‘ä¸€ä¸‹ï¼Œç£ç›˜ç©ºé—´æ„Ÿè§‰è¿˜æ˜¯å¾—æ¸…ç†ä¸‹)